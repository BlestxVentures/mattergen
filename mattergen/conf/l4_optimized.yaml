hydra:
  run:
    dir: ${oc.env:OUTPUT_DIR,outputs/singlerun/${now:%Y-%m-%d}/${now:%H-%M-%S}}

defaults:
  - data_module: mp_20
  - trainer: default
  - lightning_module: default
  - adapter: default

trainer:
  max_epochs: 200
  precision: 16  # Enable mixed precision training
  accumulate_grad_batches: 2  # Reduced from 4 since we can use larger batches
  devices: 1
  num_nodes: 1
  strategy:
    _target_: pytorch_lightning.strategies.ddp.DDPStrategy
    find_unused_parameters: true

data_module:
  batch_size:
    # Increased batch sizes to take advantage of L4's 24GB VRAM
    train: ${eval:'(256 // ${trainer.accumulate_grad_batches}) // (${trainer.devices} * ${trainer.num_nodes})'}
    val: ${eval:'(64 // (${trainer.devices} * ${trainer.num_nodes}))'}
    test: ${eval:'(64 // (${trainer.devices} * ${trainer.num_nodes}))'}
  num_workers:
    train: 4  # Increased from 0 to improve data loading
    val: 4
    test: 4

lightning_module:
  optimizer_partial:
    lr: 5e-6
  diffusion_module:
    model:
      hidden_dim: 768  # Increased from 512
      gemnet:
        max_neighbors: 50  # Increased from 30
        max_cell_images_per_dim: 5  # Increased from 3
        num_blocks: 4  # Increased from 3
        emb_size_atom: 512  # Increased from 384
        emb_size_edge: 512  # Increased from 384
        emb_size_trip: 64  # Increased from 48
        emb_size_rbf: 16  # Increased from 12
        emb_size_cbf: 16  # Increased from 12
        emb_size_bil_trip: 64  # Increased from 48 